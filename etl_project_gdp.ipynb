{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python ETL Script for GDP Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hello, I'm João Henrique. In this project, we'll demonstrate a practical application of Python for an ETL (Extract, Transform, Load) process. Our focus is on extracting Global GDP data, transforming it, and loading it into both a CSV file and a MySQL database. This showcases Python's utility in web scraping, data processing, and database operations.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Data Extraction**: \n",
    "  - Extract GDP data of countries from a Wikipedia page using web scraping techniques.\n",
    "- **Data Transformation**: \n",
    "  - Process and convert the GDP data into a more usable format.\n",
    "- **Data Load**: \n",
    "  - Load the transformed data into a CSV file and a MySQL database.\n",
    "- **Query Data**: \n",
    "  - Perform SQL queries to retrieve and analyze the data.\n",
    "\n",
    "## Technical Aspects\n",
    "\n",
    "- **Libraries Used**: \n",
    "  - `requests`, `pandas`, `BeautifulSoup`, `lxml`, `numpy`, `mysql.connector`.\n",
    "- **Data Source**: \n",
    "  - Wikipedia page on countries by GDP (nominal).\n",
    "\n",
    "## Execution Guide\n",
    "\n",
    "- **Setting Up**: \n",
    "  - Ensure all required Python libraries are installed.\n",
    "  - Set up MySQL database credentials and parameters as per your environment.\n",
    "\n",
    "- **Running the Code**: \n",
    "  - The script is divided into functions, each handling a part of the ETL process.\n",
    "  - Run the entire script to see the ETL process or step through each function individually.\n",
    "\n",
    "- **Logging**: \n",
    "  - Logs are written to `log_file.txt`, useful for debugging or tracking the process.\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "- **Extract Function**: \n",
    "  - Scrapes the webpage and extracts the GDP data.\n",
    "- **Transform Function**: \n",
    "  - Transforms the data into a more analysis-friendly format.\n",
    "- **Load to CSV Function**: \n",
    "  - Saves the data into a CSV file.\n",
    "- **Load to Database Function**: \n",
    "  - Inserts the data into a MySQL database.\n",
    "- **Query and Logging Functions**: \n",
    "  - Facilitates querying the database and logging the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Requirements\n",
    "\n",
    "To successfully run this notebook, the following software and libraries are required:\n",
    "\n",
    "### Software\n",
    "- **Python (version 3.11.3)**: The Python programming language, essential for running the code.\n",
    "- **MySQL (version 8.0.34)**: The database workbench used for storing and managing data.\n",
    "\n",
    "### Python Libraries\n",
    "- **pandas (version 2.0.3)**: A powerful data manipulation and analysis library.\n",
    "- **requests (version 2.31.0)**: Used for making HTTP requests, essential for web scraping.\n",
    "- **mysql.connector (version 8.2.0)**: Enables connection and interaction with MySQL databases.\n",
    "- **numpy (np)**: Adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- **datetime**: Provides classes for manipulating dates and times.\n",
    "- **BeautifulSoup**: A library for pulling data out of HTML and XML files, used for web scraping.\n",
    "- **Numpy**: A library for handling mathematical functions with precision and speed\n",
    "\n",
    "These libraries can be installed using pip:\n",
    "```bash\n",
    "pip install requests pandas numpy mysql-connector-python beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29\"\n",
    "CSV_PATH = 'Countries_by_GDP.csv'\n",
    "SQL_CONNECTION = ['127.0.0.1', 'root', '112104', '6666']\n",
    "LOG_PATH = 'log_file.text'\n",
    "TABLE_ATTRIBS = ['Country','GDP_USD_millions']\n",
    "DB_NAME = 'World_Economies'\n",
    "TABLE_NAME = 'Countries_by_GDP'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "Each part of the ETL process is encapsulated in a function. This makes the code easier to understand and maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `log_progress`\n",
    "\n",
    "### Purpose\n",
    "Logs a message with a timestamp to a specified log file.\n",
    "\n",
    "### Process\n",
    "1. **Timestamp Generation**: Generates a timestamp in the specified format.\n",
    "2. **Writing to Log File**: Appends the message along with the timestamp to the log file.\n",
    "\n",
    "### Parameters\n",
    "- `message`: The message to be logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    timestamp_format = \"%Y-%h-%d-%H:%M:%S\"\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(LOG_PATH,\"a\") as f:\n",
    "        f.write(timestamp+\",\"+message+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just a function to show the log in the end of this documment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_log():\n",
    "    f = open(LOG_PATH, 'r')\n",
    "    file_contents = f.read()\n",
    "    print (file_contents)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data from HTML Tables\n",
    "\n",
    "This function is designed to parse and extract data from HTML tables on a specified webpage.\n",
    "\n",
    "## Function Description\n",
    "\n",
    "### Extracting Data from HTML Tables:\n",
    "\n",
    "1. **Fetching the Webpage**: The function begins by making a GET request to the URL provided in the `url` argument. The HTML content of the webpage is then loaded into a `BeautifulSoup` object for parsing.\n",
    "\n",
    "2. **Parsing the HTML**: The function locates all table bodies (`<tbody>`) in the HTML content. Specifically, it focuses on the third table body, as indexed by `tables[2]`.\n",
    "\n",
    "3. **Extracting Table Data**: It iterates over all table rows (`<tr>`) within the targeted table. For each row, it finds all data cells (`<td>`) and checks if they meet certain conditions (e.g., not empty, contains a hyperlink, and doesn't have a specific character). If conditions are met, it extracts the relevant data (country name and GDP in millions of USD) and adds it to a dataframe.\n",
    "\n",
    "4. **Creating and Updating the DataFrame**: A new dataframe is created for each row of data and then appended to the main dataframe `df`. This process continues for each row in the HTML table.\n",
    "\n",
    "5. **Returning the DataFrame**: After iterating through all the rows, the function returns the final dataframe containing the extracted data.\n",
    "\n",
    "### Parameters:\n",
    "- `url`: The URL of the webpage from which to extract the table data.\n",
    "- `table_attribs`: A list of column names for the dataframe.\n",
    "\n",
    "### Returns:\n",
    "- A pandas DataFrame containing the extracted data from the specified HTML table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url, table_attribs):\n",
    "    webpage = requests.get(url).text\n",
    "    soup = BeautifulSoup(webpage,'html.parser')\n",
    "    \n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "    \n",
    "    tables = soup.find_all('tbody')\n",
    "    rows = tables[2].find_all('tr')\n",
    "\n",
    "    for row in rows :\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            if col[0].find('a') is not None and '—' not in col[2]:\n",
    "                data_dict = {\"Country\": col[0].a.contents[0],\n",
    "                             \"GDP_USD_millions\": col[2].contents[0]}\n",
    "                df1 = pd.DataFrame(data_dict, index=[0])\n",
    "                df = pd.concat([df,df1], ignore_index=True)\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Country GDP_USD_millions\n",
      "0       United States       26,854,599\n",
      "1               China       19,373,586\n",
      "2               Japan        4,409,738\n",
      "3             Germany        4,308,854\n",
      "4               India        3,736,882\n",
      "..                ...              ...\n",
      "186  Marshall Islands              291\n",
      "187             Palau              262\n",
      "188          Kiribati              248\n",
      "189             Nauru              151\n",
      "190            Tuvalu               65\n",
      "\n",
      "[191 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df=extract(URL,TABLE_ATTRIBS)\n",
    "    log_progress(\"Data extracted successfuly\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while extracting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `transform`\n",
    "\n",
    "### Purpose\n",
    "The `transform` function is responsible for converting GDP values from millions to billions and preparing the data for further analysis or storage.\n",
    "\n",
    "### Process\n",
    "1. **Converting Data Type**: The function starts by extracting the 'GDP_USD_millions' column from the dataframe and converting the values from string to float. This involves removing commas from the numerical strings and casting them to float.\n",
    "\n",
    "2. **Scaling the Values**: The GDP values, initially in millions, are then divided by 1,000 to convert them into billions. This scaling is rounded to two decimal places for precision and readability.\n",
    "\n",
    "3. **Updating the DataFrame**: Finally, the function updates the dataframe by replacing the original 'GDP_USD_millions' column with the newly scaled values and renaming the column to 'GDP_USD_billions' to reflect the change in units.\n",
    "\n",
    "### Returns\n",
    "- A pandas DataFrame with the updated GDP values in billions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    GDP_list = df[\"GDP_USD_millions\"].tolist()\n",
    "    GDP_list = [float(\"\".join(x.split(','))) for x in GDP_list]\n",
    "    GDP_list = [np.round(x/1000,2) for x in GDP_list]\n",
    "    df[\"GDP_USD_millions\"] = GDP_list\n",
    "    df = df.rename(columns={\"GDP_USD_millions\":\"GDP_USD_billions\"})\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Country  GDP_USD_billions\n",
      "0       United States          26854.60\n",
      "1               China          19373.59\n",
      "2               Japan           4409.74\n",
      "3             Germany           4308.85\n",
      "4               India           3736.88\n",
      "..                ...               ...\n",
      "186  Marshall Islands              0.29\n",
      "187             Palau              0.26\n",
      "188          Kiribati              0.25\n",
      "189             Nauru              0.15\n",
      "190            Tuvalu              0.06\n",
      "\n",
      "[191 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "try:       \n",
    "    df=transform(df)\n",
    "    log_progress(\"Data transformed successfuly\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while transforming data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `load_to_csv`\n",
    "\n",
    "### Purpose\n",
    "The `load_to_csv` function is designed to save the processed dataframe to a CSV file, allowing for easy storage and accessibility of the data.\n",
    "\n",
    "### Process\n",
    "1. **Exporting to CSV**: The function takes the dataframe and the path where the CSV file should be saved. It then uses pandas' `to_csv` method to write the dataframe into a CSV file at the specified location.\n",
    "\n",
    "### Parameters\n",
    "- `df`: The dataframe to be saved.\n",
    "- `csv_path`: The file path where the CSV file will be stored.\n",
    "\n",
    "### Returns\n",
    "- This function does not return a value; its primary purpose is to save the dataframe as a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df):\n",
    "    df.to_csv(CSV_PATH)\n",
    "    print(f\"data loaded to:{CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded to:Countries_by_GDP.csv\n"
     ]
    }
   ],
   "source": [
    "try:       \n",
    "    load_to_csv(df)\n",
    "    log_progress(\"Data loaded to CSV successfuly\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while loading data to CSV: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `connect_database`\n",
    "\n",
    "### Purpose\n",
    "The `connect_database` function establishes a connection to a MySQL database using provided connection parameters.\n",
    "\n",
    "### Process\n",
    "1. **Establishing Connection**: Utilizes the `mysql.connector.connect` method to create a connection to the MySQL database. The connection parameters (host, user, password, and port) are specified in the `sql_connection` list.\n",
    "\n",
    "### Parameters\n",
    "- `sql_connection`: A list containing connection parameters (host, user, password, port).\n",
    "\n",
    "### Returns\n",
    "- A connection object to the MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_database():\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "        \n",
    "    host=SQL_CONNECTION[0],\n",
    "    user=SQL_CONNECTION[1],\n",
    "    password=SQL_CONNECTION[2],\n",
    "    port=SQL_CONNECTION[3]\n",
    "    )   \n",
    "    print(mydb)\n",
    "    return mydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000002A65955BC10>\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    connect_database()\n",
    "    log_progress(\"Database successfuly connected\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while connecting to DB: {e}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `load_to_db`\n",
    "\n",
    "### Purpose\n",
    "The `load_to_db` function is responsible for creating a database schema and table, and then loading data from a pandas DataFrame into the MySQL database table.\n",
    "\n",
    "### Process\n",
    "1. **Database Connection**: Connects to the MySQL database using the `connect_database` function.\n",
    "2. **Schema and Table Creation**: Creates a new schema and table in the database if they do not exist. The table structure is defined based on the dataframe's columns.\n",
    "3. **Data Insertion**: Converts the dataframe into records and inserts each record into the database table using an INSERT statement.\n",
    "\n",
    "### Parameters\n",
    "- `df`: The pandas DataFrame to be loaded into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(df):\n",
    "    db = connect_database()\n",
    "    cursor = db.cursor()\n",
    "    print(\"Database connection established.\")\n",
    "            \n",
    "    column_names = df.columns.tolist()\n",
    "    data_records = df.to_records(index=False)\n",
    "\n",
    "    # Create the schema and table\n",
    "    cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS `{DB_NAME}`\")\n",
    "    cursor.execute(f\"USE `{DB_NAME}`\")\n",
    "    print(f\"Schema `{DB_NAME}` accessed/created.\")\n",
    "    \n",
    "    columns_sql = ', '.join([f\"`{col}` VARCHAR(255)\" if df[col].dtype == 'object' else f\"`{col}` FLOAT\" for col in column_names])\n",
    "    cursor.execute(f\"CREATE TABLE IF NOT EXISTS `{TABLE_NAME}` (id INT AUTO_INCREMENT PRIMARY KEY, {columns_sql})\")\n",
    "    print(f\"Table `{TABLE_NAME}` accessed/created.\")\n",
    "            \n",
    "    # Prepare the INSERT statement\n",
    "    placeholders = ', '.join(['%s'] * len(column_names))\n",
    "    DML = f\"INSERT INTO `{TABLE_NAME}` ({', '.join([f'`{col}`' for col in column_names])}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Insert each record\n",
    "    for record in data_records:\n",
    "        values = tuple(record[col] for col in column_names)\n",
    "        cursor.execute(DML, values)\n",
    "\n",
    "    db.commit()\n",
    "    db.close()\n",
    "    print(\"Data inserted successfully into the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000002A658D45190>\n",
      "Database connection established.\n",
      "Schema `World_Economies` accessed/created.\n",
      "Table `Countries_by_GDP` accessed/created.\n",
      "Data inserted successfully into the database.\n"
     ]
    }
   ],
   "source": [
    "try:       \n",
    "    load_to_db(df)\n",
    "    log_progress(\"Data loaded to DB successfuly\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while loading data to DB: {e}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Description for `run_query`\n",
    "\n",
    "### Purpose\n",
    "Executes a specified SQL query on the connected MySQL database and prints the query result.\n",
    "\n",
    "### Process\n",
    "1. **Database Connection**: Connects to the MySQL database using the `connect_database` function.\n",
    "2. **Executing Query**: Executes the provided SQL query and retrieves the results using pandas' `read_sql` function.\n",
    "\n",
    "### Parameters\n",
    "- `query_statement`: The SQL query to be executed.\n",
    "\n",
    "### Returns\n",
    "- The executed query statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_statement):\n",
    "    db = connect_database()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(f\"Use {DB_NAME}\")\n",
    "    print(pd.read_sql(query_statement,db))\n",
    "    return query_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the query yourself!\n",
    "\n",
    "You can know more about querys in my <a href=\"https://balenciagaa.notion.site/SQL-1b15be8f10d74695a8de7bc860292f1b?pvs=73\">SQL tutorial</a> (It's in portuguese right now but i will fix soon): \n",
    "\n",
    "\n",
    "## Query examples:\n",
    "\n",
    "**Retrieve Records with Specific Conditions**\n",
    "\n",
    "`SELECT * FROM Countries_by_GDP WHERE GDP_USD_billions > 50 ORDER BY GDP_USD_billions DESC`\n",
    "\n",
    "**Calculate Average GPD**\n",
    "\n",
    "`SELECT AVG(GDP_USD_billions) AS average_GDP FROM Countries_by_GDP`\n",
    "\n",
    "**Pick the top 5 GDP**\n",
    "\n",
    "`SELECT Country FROM Countries_by_GDP ORDER BY GDP_USD_billions DESC LIMIT 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000002A658D46E90>\n",
      "        id           Country  GDP_USD_billions\n",
      "0        1     United States          26854.60\n",
      "1        2             China          19373.60\n",
      "2        3             Japan           4409.74\n",
      "3        4           Germany           4308.85\n",
      "4        5             India           3736.88\n",
      "...    ...               ...               ...\n",
      "1332  1333  Marshall Islands              0.29\n",
      "1333  1334             Palau              0.26\n",
      "1334  1335          Kiribati              0.25\n",
      "1335  1336             Nauru              0.15\n",
      "1336  1337            Tuvalu              0.06\n",
      "\n",
      "[1337 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaoh\\AppData\\Local\\Temp\\ipykernel_17848\\1288848911.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  print(pd.read_sql(query_statement,db))\n"
     ]
    }
   ],
   "source": [
    "try:       \n",
    "    quey_statement = run_query(f\"SELECT * FROM Countries_by_GDP\")\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error while querying data: {e}\")\n",
    "finally:\n",
    "        log_progress(f\"Query made: {quey_statement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the log progess of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-Dec-21-22:07:22,Data extracted successfuly\n",
      "2023-Dec-21-22:07:27,Data transformed successfuly\n",
      "2023-Dec-21-22:08:50,Data loaded to CSV successfuly\n",
      "2023-Dec-21-22:09:17,Database successfuly connected\n",
      "2023-Dec-21-22:12:15,Data loaded to DB successfuly\n",
      "2023-Dec-21-22:12:51,Query made: SELECT * FROM Countries_by_GDP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "open_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations\n",
    "\n",
    "### Summary of the Project\n",
    "In this ETL project, we successfully developed and executed a pipeline to extract, transform, and load data regarding global GDP figures. The process involved retrieving data from a web source, manipulating the data for consistency and scale, and ultimately storing it in a MySQL database for further analysis and retrieval.\n",
    "\n",
    "### Key Learnings\n",
    "- **Python's Adaptability in Data Extraction**: This project showcased Python's strength in web scraping and data extraction, demonstrating its ability to interact with web content and parse HTML.\n",
    "- **Complex Data Transformation**: We highlighted the importance and methods of transforming raw data into a more analyzable format, particularly converting GDP figures from millions to billions.\n",
    "- **Database Interaction and Data Storage**: The project provided practical experience in loading data into a MySQL database, emphasizing the significance of data storage in structured formats for efficient retrieval and analysis.\n",
    "- **Dynamic Data Handling**: Addressing various challenges, such as dynamic data structures in web tables and ensuring seamless database connectivity, was a crucial learning aspect.\n",
    "\n",
    "### Conclusion\n",
    "This ETL project served as an excellent practical application of data extraction, transformation, and loading using Python. It allowed us to delve deep into aspects of data handling, from web scraping to database management, underlining the vital role of ETL processes in making data useful for analysis and decision-making.\n",
    "\n",
    "I hope this project has provided valuable insights into ETL processes, demonstrating the versatility of Python in handling and manipulating data for meaningful outcomes. Thank you for engaging with this project, and I look forward to any future collaborations or discussions on similar data-driven initiatives.\n",
    "\n",
    "- Special thanks to IBM, without the awesome Data Engeneering Course this project would be more difficult to do ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
